{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:16:42.513611Z",
     "iopub.status.busy": "2025-04-05T00:16:42.513209Z",
     "iopub.status.idle": "2025-04-05T00:16:54.959833Z",
     "shell.execute_reply": "2025-04-05T00:16:54.958548Z",
     "shell.execute_reply.started": "2025-04-05T00:16:42.513580Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 4)) (3.7.5)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 5)) (0.12.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 6)) (4.67.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 9)) (3.3.1)\n",
      "Collecting tree-sitter (from -r /kaggle/input/requirements-txt/requirements.txt (line 10))\n",
      "  Downloading tree_sitter-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n",
      "Collecting tree-sitter-python (from -r /kaggle/input/requirements-txt/requirements.txt (line 11))\n",
      "  Downloading tree_sitter_python-0.23.6-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting tree-sitter-java (from -r /kaggle/input/requirements-txt/requirements.txt (line 12))\n",
      "  Downloading tree_sitter_java-0.23.5-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 15)) (1.2.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 16)) (4.47.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 17)) (1.2.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 18)) (2.5.1+cu121)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 19)) (0.14.0)\n",
      "Collecting evaluate (from -r /kaggle/input/requirements-txt/requirements.txt (line 22))\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting rouge_score (from -r /kaggle/input/requirements-txt/requirements.txt (line 23))\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting sacrebleu (from -r /kaggle/input/requirements-txt/requirements.txt (line 24))\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 25)) (0.19.1)\n",
      "Collecting streamlit (from -r /kaggle/input/requirements-txt/requirements.txt (line 28))\n",
      "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 31)) (0.45.1)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 34)) (0.20.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->-r /kaggle/input/requirements-txt/requirements.txt (line 2)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->-r /kaggle/input/requirements-txt/requirements.txt (line 2)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->-r /kaggle/input/requirements-txt/requirements.txt (line 2)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->-r /kaggle/input/requirements-txt/requirements.txt (line 2)) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->-r /kaggle/input/requirements-txt/requirements.txt (line 2)) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->-r /kaggle/input/requirements-txt/requirements.txt (line 2)) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /kaggle/input/requirements-txt/requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /kaggle/input/requirements-txt/requirements.txt (line 3)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /kaggle/input/requirements-txt/requirements.txt (line 3)) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /kaggle/input/requirements-txt/requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /kaggle/input/requirements-txt/requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /kaggle/input/requirements-txt/requirements.txt (line 4)) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /kaggle/input/requirements-txt/requirements.txt (line 4)) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /kaggle/input/requirements-txt/requirements.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /kaggle/input/requirements-txt/requirements.txt (line 4)) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /kaggle/input/requirements-txt/requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (0.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (6.0.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r /kaggle/input/requirements-txt/requirements.txt (line 15)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r /kaggle/input/requirements-txt/requirements.txt (line 15)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r /kaggle/input/requirements-txt/requirements.txt (line 15)) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /kaggle/input/requirements-txt/requirements.txt (line 16)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /kaggle/input/requirements-txt/requirements.txt (line 16)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /kaggle/input/requirements-txt/requirements.txt (line 16)) (0.4.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r /kaggle/input/requirements-txt/requirements.txt (line 17)) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r /kaggle/input/requirements-txt/requirements.txt (line 18)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r /kaggle/input/requirements-txt/requirements.txt (line 18)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r /kaggle/input/requirements-txt/requirements.txt (line 18)) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r /kaggle/input/requirements-txt/requirements.txt (line 18)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r /kaggle/input/requirements-txt/requirements.txt (line 18)) (1.3.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score->-r /kaggle/input/requirements-txt/requirements.txt (line 23)) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score->-r /kaggle/input/requirements-txt/requirements.txt (line 23)) (3.2.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score->-r /kaggle/input/requirements-txt/requirements.txt (line 23)) (1.17.0)\n",
      "Collecting portalocker (from sacrebleu->-r /kaggle/input/requirements-txt/requirements.txt (line 24))\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r /kaggle/input/requirements-txt/requirements.txt (line 24)) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r /kaggle/input/requirements-txt/requirements.txt (line 24)) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r /kaggle/input/requirements-txt/requirements.txt (line 24)) (5.3.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (3.20.3)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (2.11.0a2)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (75.1.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28)) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28)) (5.5.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28)) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28)) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28)) (6.0.0)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28))\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28)) (6.3.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28)) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28)) (1.18.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (2.29.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r /kaggle/input/requirements-txt/requirements.txt (line 18)) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r /kaggle/input/requirements-txt/requirements.txt (line 9)) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->-r /kaggle/input/requirements-txt/requirements.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->-r /kaggle/input/requirements-txt/requirements.txt (line 2)) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->-r /kaggle/input/requirements-txt/requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->-r /kaggle/input/requirements-txt/requirements.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r /kaggle/input/requirements-txt/requirements.txt (line 25)) (5.0.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->-r /kaggle/input/requirements-txt/requirements.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r /kaggle/input/requirements-txt/requirements.txt (line 28)) (0.22.3)\n",
      "Downloading tree_sitter-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (574 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m574.3/574.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tree_sitter_python-0.23.6-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tree_sitter_java-0.23.5-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=a7f8462158c94cecf0f77c50172d59599728ef6befe49891a7b2d433eb662ccb\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: tree-sitter-python, tree-sitter-java, tree-sitter, portalocker, pydeck, streamlit, sacrebleu, rouge_score, evaluate\n",
      "Successfully installed evaluate-0.4.3 portalocker-3.1.1 pydeck-0.9.1 rouge_score-0.1.2 sacrebleu-2.5.1 streamlit-1.44.1 tree-sitter-0.24.0 tree-sitter-java-0.23.5 tree-sitter-python-0.23.6\n",
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r /kaggle/input/requirements-txt/requirements.txt\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-05T00:16:54.961779Z",
     "iopub.status.busy": "2025-04-05T00:16:54.961411Z",
     "iopub.status.idle": "2025-04-05T00:17:25.459283Z",
     "shell.execute_reply": "2025-04-05T00:17:25.458290Z",
     "shell.execute_reply.started": "2025-04-05T00:16:54.961741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# WandB\n",
    "import wandb\n",
    "\n",
    "# AST\n",
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_python\n",
    "import tree_sitter_java\n",
    "\n",
    "## AST Graphing\n",
    "import graphviz\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# Datasets\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, concatenate_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python loading\n",
    "python_dataset = DatasetDict({\n",
    "    'train': load_dataset('code_search_net', 'python', split='train[:60000]', trust_remote_code=True),\n",
    "    'validation': load_dataset('code_search_net', 'python', split='validation[:7000]', trust_remote_code=True),\n",
    "    'test': load_dataset('code_search_net', 'python', split='test[:3500]', trust_remote_code=True)\n",
    "})\n",
    "\n",
    "python_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Java Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:19:39.578278Z",
     "iopub.status.busy": "2025-04-05T00:19:39.577847Z",
     "iopub.status.idle": "2025-04-05T00:21:58.330965Z",
     "shell.execute_reply": "2025-04-05T00:21:58.330123Z",
     "shell.execute_reply.started": "2025-04-05T00:19:39.578239Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b567e4bb134663b542fff64baea324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "java.zip:   0%|          | 0.00/1.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbb5c04cc484a498155bb74797e3381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/454451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cea34f30f214679b5c83a10a2c12f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/26909 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc12b990c3d40848953f4ac7acb2e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/15328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 7000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 3500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Java loading\n",
    "java_dataset = DatasetDict({\n",
    "    'train': load_dataset('code_search_net', 'java', split='train[:60000]', trust_remote_code=True),\n",
    "    'validation': load_dataset('code_search_net', 'java', split='validation[:7000]', trust_remote_code=True),\n",
    "    'test': load_dataset('code_search_net', 'java', split='test[:3500]', trust_remote_code=True)\n",
    "})\n",
    "\n",
    "java_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug and Test modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:21:58.332188Z",
     "iopub.status.busy": "2025-04-05T00:21:58.331849Z",
     "iopub.status.idle": "2025-04-05T00:21:58.335938Z",
     "shell.execute_reply": "2025-04-05T00:21:58.335087Z",
     "shell.execute_reply.started": "2025-04-05T00:21:58.332163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# set to False for full training config\n",
    "debug = False\n",
    "# set tp True for enabling testing blocks\n",
    "test_run = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:21:58.337066Z",
     "iopub.status.busy": "2025-04-05T00:21:58.336785Z",
     "iopub.status.idle": "2025-04-05T00:21:59.675759Z",
     "shell.execute_reply": "2025-04-05T00:21:59.674747Z",
     "shell.execute_reply.started": "2025-04-05T00:21:58.337042Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'private static int[][] findAromaticRings(int[][] cycles, int[] contribution, int[] dbs) {\\n\\n        // loop control variables, the while loop continual checks all cycles\\n        // until no changes are found\\n        boolean found;\\n        boolean[] checked = new boolean[cycles.length];\\n\\n        // stores the aromatic atoms as a bit set and the aromatic bonds as\\n        // a hash set. the aromatic bonds are the result of this method but the\\n        // aromatic atoms are needed for checking each ring\\n        final boolean[] aromaticAtoms = new boolean[contribution.length];\\n\\n        final List<int[]> ringsOfSize6 = new ArrayList<int[]>();\\n        final List<int[]> ringsOfSize5 = new ArrayList<int[]>();\\n\\n        do {\\n            found = false;\\n            for (int i = 0; i < cycles.length; i++) {\\n\\n                // note paths are closed walks and repeat first/last vertex so\\n                // the true length is one less\\n                int[] cycle = cycles[i];\\n                int len = cycle.length - 1;\\n\\n                if (checked[i]) continue;\\n\\n                if (isAromaticRing(cycle, contribution, dbs, aromaticAtoms)) {\\n                    checked[i] = true;\\n                    found |= true;\\n                    for (int j = 0; j < len; j++) {\\n                        aromaticAtoms[cycle[j]] = true;\\n                    }\\n                    if (len == 6)\\n                        ringsOfSize6.add(cycle);\\n                    else if (len == 5) ringsOfSize5.add(cycle);\\n\\n                }\\n            }\\n        } while (found);\\n\\n        List<int[]> rings = new ArrayList<int[]>();\\n        rings.addAll(ringsOfSize6);\\n        rings.addAll(ringsOfSize5);\\n\\n        return rings.toArray(new int[rings.size()][]);\\n    }'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset = DatasetDict({\n",
    "    'train': concatenate_datasets([python_dataset['train'], java_dataset['train']]),\n",
    "    'validation': concatenate_datasets([python_dataset['validation'], java_dataset['validation']]),\n",
    "    'test': concatenate_datasets([python_dataset['test'], java_dataset['test']])\n",
    "})\n",
    "\n",
    "if debug:\n",
    "    combined_dataset[\"train\"] = combined_dataset[\"train\"].select(range(200))\n",
    "    combined_dataset[\"validation\"] = combined_dataset[\"validation\"].select(range(50))\n",
    "    combined_dataset[\"test\"] = combined_dataset[\"test\"].select(range(50))\n",
    "\n",
    "combined_dataset['train'] = combined_dataset['train'].shuffle(seed=42)\n",
    "combined_dataset['validation'] = combined_dataset['validation'].shuffle(seed=42)\n",
    "combined_dataset['test'] = combined_dataset['test'].shuffle(seed=42)\n",
    "\n",
    "combined_dataset['train']['func_code_string'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:21:59.677257Z",
     "iopub.status.busy": "2025-04-05T00:21:59.676854Z",
     "iopub.status.idle": "2025-04-05T00:21:59.698453Z",
     "shell.execute_reply": "2025-04-05T00:21:59.697312Z",
     "shell.execute_reply.started": "2025-04-05T00:21:59.677212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the languages\n",
    "PY_LANGUAGE = Language(tree_sitter_python.language())\n",
    "JAVA_LANGUAGE = Language(tree_sitter_java.language())\n",
    "\n",
    "# Initialize the parsers by passing the language\n",
    "python_parser = Parser(PY_LANGUAGE)\n",
    "java_parser = Parser(JAVA_LANGUAGE)\n",
    "\n",
    "def parse_code_to_ast(code, language):\n",
    "    if language.lower() == 'python':\n",
    "        parser = python_parser\n",
    "    elif language.lower() == 'java':\n",
    "        parser = java_parser\n",
    "    tree = parser.parse(bytes(code, 'utf8'))\n",
    "    return tree\n",
    "\n",
    "def sbt_traverse(node):\n",
    "    \"\"\"\n",
    "    Recursively traverse the AST node using an SBT (Structure-Based Traversal) method.\n",
    "    This function outputs a list of tokens with explicit start and end markers for each node.\n",
    "    \"\"\"\n",
    "    # Add a start marker for the current node\n",
    "    sequence = [f\"<{node.type}>\"]\n",
    "    # Recursively traverse each child and extend the sequence\n",
    "    for child in node.children:\n",
    "        sequence.extend(sbt_traverse(child))\n",
    "    # Add an end marker for the current node\n",
    "    sequence.append(f\"</{node.type}>\")\n",
    "    return sequence\n",
    "\n",
    "# Visualize AST Graph\n",
    "def visualize_ast(tree):\n",
    "    dot = graphviz.Digraph(format=\"png\")\n",
    "    \n",
    "    def add_nodes_edges(node, parent_id=None):\n",
    "        node_id = str(id(node))\n",
    "        dot.node(node_id, label=node.type)  # Add the node with its type as label\n",
    "\n",
    "        if parent_id:\n",
    "            dot.edge(parent_id, node_id)  # Add an edge from the parent to this node\n",
    "        \n",
    "        for child in node.children:\n",
    "            add_nodes_edges(child, node_id)\n",
    "    \n",
    "    add_nodes_edges(tree.root_node)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: AST Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:21:59.701297Z",
     "iopub.status.busy": "2025-04-05T00:21:59.700969Z",
     "iopub.status.idle": "2025-04-05T00:21:59.719099Z",
     "shell.execute_reply": "2025-04-05T00:21:59.717998Z",
     "shell.execute_reply.started": "2025-04-05T00:21:59.701267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    # Retrieve code strings from your datasets (for testing)\n",
    "    python_code = python_dataset['train']['func_code_string'][2]\n",
    "    java_code = java_dataset['train']['func_code_string'][2]\n",
    "    \n",
    "    # Parse the code to AST trees\n",
    "    python_tree = python_parser.parse(bytes(python_code, \"utf8\"))\n",
    "    java_tree = java_parser.parse(bytes(java_code, \"utf8\"))\n",
    "    \n",
    "    # Generate AST Visualization for Python code sample\n",
    "    ast_viz = visualize_ast(python_tree)\n",
    "    ast_viz.render(\"ast_visualization\", format=\"png\", view=True)\n",
    "    \n",
    "    # Print the basic AST representation\n",
    "    print(\"Python AST:\")\n",
    "    print(str(python_tree.root_node))\n",
    "    print(\"\\nJava AST:\")\n",
    "    print(str(java_tree.root_node))\n",
    "    \n",
    "    # Generate SBT sequences from the ASTs\n",
    "    python_sbt_sequence = \" \".join(sbt_traverse(python_tree.root_node))\n",
    "    java_sbt_sequence = \" \".join(sbt_traverse(java_tree.root_node))\n",
    "    \n",
    "    # Print the SBT sequences\n",
    "    print(\"\\nPython SBT Sequence:\")\n",
    "    print(python_sbt_sequence)\n",
    "    print(\"\\nJava SBT Sequence:\")\n",
    "    print(java_sbt_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AST Integration, Masking & Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:21:59.720813Z",
     "iopub.status.busy": "2025-04-05T00:21:59.720523Z",
     "iopub.status.idle": "2025-04-05T00:22:03.409994Z",
     "shell.execute_reply": "2025-04-05T00:22:03.409134Z",
     "shell.execute_reply.started": "2025-04-05T00:21:59.720789Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e678bde3ff24144a8f1e84dcc8101f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f2f93483ce4f84aeae0cb26ccc4790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/703k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4170a1e013c24d8b90c82c49d9c7358b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad2bdc7533d4fce9d16a559b3d4b43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190d91acf8bc49b7ba7de33c8ad86a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/12.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_func_name(code_str: str, func_name: str, lang: str) -> str:\n",
    "    lang = lang.lower()\n",
    "\n",
    "    if lang == 'python':\n",
    "        pattern = rf\"(def\\s+)({re.escape(func_name)})(\\s*\\()\"\n",
    "        return re.sub(pattern, r\"\\1<extra_id_0>\\3\", code_str, count=1)\n",
    "    \n",
    "    elif lang == 'java':\n",
    "        pattern = rf\"(?<!\\w){re.escape(func_name)}(?=\\s*\\()\"\n",
    "        return re.sub(pattern, \"<extra_id_0>\", code_str, count=1)\n",
    "\n",
    "    else:\n",
    "        return code_str\n",
    "\n",
    "def test_real_python_samples(dataset, num_samples=3):\n",
    "    print(\"=== REAL PYTHON SAMPLES ===\")\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        full_func_name = dataset['train'][i]['func_name'] \n",
    "        method_name = full_func_name.split('.')[-1] \n",
    "        code = dataset['train'][i]['func_code_string']\n",
    "        \n",
    "        print(f\"--- Sample #{i} ---\")\n",
    "        print(f\"Original Function Name: {full_func_name}\")\n",
    "        print(\"\\nOriginal Code:\\n\", code)\n",
    "        print(\"\\nMasked Code:\\n\", mask_func_name(code, method_name, lang=\"python\"))\n",
    "        print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "def test_real_java_samples(dataset, num_samples=3):\n",
    "    print(\"=== REAL JAVA SAMPLES ===\")\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        full_func_name = dataset['train'][i]['func_name']\n",
    "        method_name = full_func_name.split('.')[-1]\n",
    "        code = dataset['train'][i]['func_code_string']\n",
    "        \n",
    "        print(f\"--- Sample #{i} ---\")\n",
    "        print(f\"Original Function Name: {full_func_name}\")\n",
    "        print(\"\\nOriginal Code:\\n\", code)\n",
    "        print(\"\\nMasked Code:\\n\", mask_func_name(code, method_name, lang=\"java\"))\n",
    "        print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "\n",
    "def inspect_samples(dataset, lang: str, num_samples: int = 5):\n",
    "    print(f\"\\n=== {lang.upper()} SAMPLE VERIFICATION ===\\n\")\n",
    "    for i in range(num_samples):\n",
    "        sample = dataset['train'][i]\n",
    "        code = sample['func_code_string']\n",
    "        full_name = sample['func_name']\n",
    "        method_name = full_name.split('.')[-1]\n",
    "\n",
    "        masked_code = mask_func_name(code, method_name, lang)\n",
    "        dummy_ast = \"<AST> dummy AST </AST>\"\n",
    "        combined_input = masked_code + \" \" + dummy_ast\n",
    "        tokens = tokenizer.tokenize(combined_input)\n",
    "\n",
    "        print(f\"--- Sample #{i} ---\")\n",
    "        print(f\"Original Function Name: {full_name}\")\n",
    "        print(\"\\nOriginal Code:\\n\", code)\n",
    "        print(\"\\nMasked Code:\\n\", masked_code)\n",
    "        print(\"\\nFinal Combined Input:\\n\", combined_input)\n",
    "        print(\"\\nTokenized Input:\\n\", tokens)\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "def preprocess(examples):\n",
    "    combined_inputs = []\n",
    "    combined_labels = []\n",
    "    \n",
    "    # Iterate over each example\n",
    "    for code, target, lang in zip(examples['func_code_string'], examples['func_name'], examples['language']):\n",
    "         # Extract method name (in case it's fully qualified like Class.method)\n",
    "        method_name = target.split('.')[-1]\n",
    "        # Mask function name in definition\n",
    "        masked_code = mask_func_name(code, method_name, lang)\n",
    "\n",
    "        tree = parse_code_to_ast(code, lang)\n",
    "        root_node = tree.root_node\n",
    "        ast_features = sbt_traverse(root_node)\n",
    "        \n",
    "        ast_string = \"<AST> \" + \" \".join(ast_features) + \" </AST>\" # Wrapping the AST features with <AST> and </AST>.\n",
    "        combined_input = masked_code + \" \" + ast_string # Combining code with AST features\n",
    "\n",
    "        combined_inputs.append(combined_input)\n",
    "        combined_labels.append(method_name) # Extract the method name from the full path\n",
    "    \n",
    "    # Tokenize the combined input and targets\n",
    "    model_inputs = tokenizer(combined_inputs, max_length=1024, truncation=True, padding='max_length')\n",
    "    tokenized_labels = tokenizer(combined_labels, max_length=50, truncation=True, padding='max_length')\n",
    "    \n",
    "    model_inputs['labels'] = tokenized_labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "if debug:\n",
    "    # Run both inspections\n",
    "    inspect_samples(python_dataset, lang=\"python\", num_samples=5)\n",
    "    inspect_samples(java_dataset, lang=\"java\", num_samples=5)\n",
    "    \n",
    "    # Run the test\n",
    "    test_real_java_samples(java_dataset, num_samples=5)\n",
    "    test_real_python_samples(python_dataset, num_samples=5)\n",
    "    \n",
    "    print(\"<extra_id_0>\" in tokenizer.get_vocab())\n",
    "    print(\"<mask>\" in tokenizer.get_vocab())\n",
    "    print(\"Token ID for <extra_id_0>:\", tokenizer.convert_tokens_to_ids(\"<extra_id_0>\"))\n",
    "    print(\"All special tokens:\", tokenizer.special_tokens_map)\n",
    "    print(\"Additional special tokens:\", tokenizer.additional_special_tokens)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
    "special_tokens = {\"additional_special_tokens\": [\"<AST>\", \"</AST>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:22:03.411145Z",
     "iopub.status.busy": "2025-04-05T00:22:03.410842Z",
     "iopub.status.idle": "2025-04-05T00:29:01.556687Z",
     "shell.execute_reply": "2025-04-05T00:29:01.555761Z",
     "shell.execute_reply.started": "2025-04-05T00:22:03.411121Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess at 0x7c1ef61820e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7a971684c345f4a6e98d7aaf1ca3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e15f7473084627b07ed93b0caeac9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9facd3bcffc4b918383cb7721a6c31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = combined_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:29:01.558761Z",
     "iopub.status.busy": "2025-04-05T00:29:01.558463Z",
     "iopub.status.idle": "2025-04-05T00:29:01.565350Z",
     "shell.execute_reply": "2025-04-05T00:29:01.564253Z",
     "shell.execute_reply.started": "2025-04-05T00:29:01.558736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    num_samples_to_show = 5\n",
    "    \n",
    "    for idx in range(num_samples_to_show):\n",
    "        print(f\"\\n===== Sample {idx + 1} =====\")\n",
    "    \n",
    "        # Print decoded input (with masking, i.e., function body with <extra_id_0>)\n",
    "        input_ids = tokenized_dataset[\"train\"][idx][\"input_ids\"]\n",
    "        decoded_input = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "        print(\"Masked Input Code:\\n\", decoded_input)\n",
    "    \n",
    "        # Print decoded label (method name target)\n",
    "        label_ids = tokenized_dataset[\"train\"][idx][\"labels\"]\n",
    "        decoded_label = tokenizer.decode(\n",
    "            [id for id in label_ids if id != tokenizer.pad_token_id],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        print(\"Target Method Name:\", decoded_label)\n",
    "    \n",
    "        # Optional: show original method name from combined dataset (if available)\n",
    "        if \"func_name\" in combined_dataset[\"train\"].features:\n",
    "            original_name = combined_dataset[\"train\"][idx][\"func_name\"]\n",
    "            print(\"Original Method Name:\", original_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:29:01.567031Z",
     "iopub.status.busy": "2025-04-05T00:29:01.566637Z",
     "iopub.status.idle": "2025-04-05T00:29:01.770344Z",
     "shell.execute_reply": "2025-04-05T00:29:01.769323Z",
     "shell.execute_reply.started": "2025-04-05T00:29:01.566993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    # Show sample\n",
    "    \n",
    "    print(tokenized_dataset[\"train\"][0])\n",
    "    print(tokenizer.decode(tokenized_dataset[\"train\"][0][\"input_ids\"]))\n",
    "    \n",
    "    print(tokenized_dataset[\"train\"][0])\n",
    "    print(tokenizer.decode(tokenized_dataset[\"train\"][0][\"input_ids\"]))\n",
    "    \n",
    "    \n",
    "    sample_index = 0 \n",
    "    \n",
    "    # From original dataset (before masking)\n",
    "    original_func_name = combined_dataset[\"train\"][sample_index][\"func_name\"]\n",
    "    print(\"Full Function Name:\", original_func_name)\n",
    "    \n",
    "    # From label inside tokenized dataset\n",
    "    label_ids = tokenized_dataset[\"train\"][sample_index][\"labels\"]\n",
    "    label_text = tokenizer.decode([id for id in label_ids if id != tokenizer.pad_token_id], skip_special_tokens=True)\n",
    "    print(\"Target Label Text (after masking & preprocessing):\", label_text)\n",
    "    \n",
    "    label_ids = tokenized_dataset['train'][0]['labels']\n",
    "    label_text = tokenizer.decode([id for id in label_ids if id != tokenizer.pad_token_id], skip_special_tokens=True)\n",
    "    print(\"Decoded Label (method name):\", label_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W&B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make all changes to hyper-params here, pls do not change elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:29:01.771860Z",
     "iopub.status.busy": "2025-04-05T00:29:01.771499Z",
     "iopub.status.idle": "2025-04-05T00:29:01.792788Z",
     "shell.execute_reply": "2025-04-05T00:29:01.791722Z",
     "shell.execute_reply.started": "2025-04-05T00:29:01.771828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    config = {\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"num_train_epochs\": 1,\n",
    "        \"eval_steps\": 20,\n",
    "        \"save_steps\": 20,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"logging_steps\": 10,\n",
    "        \"fp16\": False,  # for smoke-test\n",
    "        \"predict_with_generate\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"logging_strategy\": \"steps\",\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"output_dir\": \"./debug_results\",\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"run_name\": \"mngast120k_smoke_test\",\n",
    "        \"model_name\": \"Salesforce/codet5-base\"\n",
    "    }\n",
    "else:\n",
    "    # Define hyperparameters in a dictionary\n",
    "    config = {\n",
    "        \"learning_rate\": 4e-7,\n",
    "        \"batch_size\": 8,\n",
    "        \"num_train_epochs\": 2,\n",
    "        \"eval_steps\": 5000,\n",
    "        \"save_steps\": 5000,\n",
    "        \"save_total_limit\": 3,\n",
    "        \"logging_steps\": 100,\n",
    "        #\"weight_decay\": 0.01,\n",
    "        \"fp16\": True,\n",
    "        \"predict_with_generate\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"logging_strategy\": \"steps\",\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"output_dir\": \"./training_results\",\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"run_name\": \"mngast120k_training\",\n",
    "        \"model_name\": \"Salesforce/codet5-base\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:03:23.112827Z",
     "iopub.status.busy": "2025-04-04T08:03:23.112499Z",
     "iopub.status.idle": "2025-04-04T08:03:36.803870Z",
     "shell.execute_reply": "2025-04-04T08:03:36.803156Z",
     "shell.execute_reply.started": "2025-04-04T08:03:23.112799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Log hyperparameters to W&B\n",
    "wandb.login(key=\"ebd5969438c4d7fbf09289ce11c991e89fcc3b5b\")\n",
    "wandb.init(project=\"Method Name Prediction\", name=\"mng_training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:03:51.842477Z",
     "iopub.status.busy": "2025-04-04T08:03:51.842189Z",
     "iopub.status.idle": "2025-04-04T08:03:51.848237Z",
     "shell.execute_reply": "2025-04-04T08:03:51.847453Z",
     "shell.execute_reply.started": "2025-04-04T08:03:51.842455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:29:01.794506Z",
     "iopub.status.busy": "2025-04-05T00:29:01.794069Z",
     "iopub.status.idle": "2025-04-05T00:29:08.629461Z",
     "shell.execute_reply": "2025-04-05T00:29:08.628225Z",
     "shell.execute_reply.started": "2025-04-05T00:29:01.794467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfe9e707f8941c598444ca986a13cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e786bd01814a3a8afaa5059e1ff2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32102, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(config[\"model_name\"])\n",
    "\n",
    "# Accounting for additional <AST> special tokens\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA - Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:29:08.631442Z",
     "iopub.status.busy": "2025-04-05T00:29:08.631066Z",
     "iopub.status.idle": "2025-04-05T00:29:08.924316Z",
     "shell.execute_reply": "2025-04-05T00:29:08.923281Z",
     "shell.execute_reply.started": "2025-04-05T00:29:08.631396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        # Encoder attention part\n",
    "        \"q\", \"k\", \"v\", \"o\",\n",
    "        # Decoder attention part\n",
    "        \"decoder.q\", \"decoder.k\", \"decoder.v\", \"decoder.o\",\n",
    "        # Feed-forward network layers\n",
    "        \"wi\", \"wo\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pls verify file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:29:08.925651Z",
     "iopub.status.busy": "2025-04-05T00:29:08.925367Z",
     "iopub.status.idle": "2025-04-05T00:29:10.341344Z",
     "shell.execute_reply": "2025-04-05T00:29:10.340312Z",
     "shell.execute_reply.started": "2025-04-05T00:29:08.925626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(\n",
    "    \"/kaggle/input\", \"checkpoint-path\", \"training_results\", \"training_results\", \"checkpoint-25000\"\n",
    ")\n",
    "model.load_adapter(checkpoint_dir, adapter_name=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:04:53.758456Z",
     "iopub.status.busy": "2025-04-04T08:04:53.758164Z",
     "iopub.status.idle": "2025-04-04T08:04:53.790563Z",
     "shell.execute_reply": "2025-04-04T08:04:53.789918Z",
     "shell.execute_reply.started": "2025-04-04T08:04:53.758434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cnfg = wandb.config\n",
    "\n",
    "# All fields called from config dictionary\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    learning_rate=cnfg.learning_rate,\n",
    "    per_device_train_batch_size=cnfg.batch_size,\n",
    "    per_device_eval_batch_size=cnfg.batch_size,\n",
    "    num_train_epochs=cnfg.num_train_epochs,\n",
    "    eval_steps=cnfg.eval_steps,\n",
    "    save_steps=cnfg.save_steps,\n",
    "    save_total_limit=cnfg.save_total_limit,\n",
    "    logging_steps=cnfg.logging_steps,\n",
    "    # weight_decay=cnfg.weight_decay,\n",
    "    fp16=cnfg.fp16,\n",
    "    predict_with_generate=cnfg.predict_with_generate,\n",
    "    load_best_model_at_end=cnfg.load_best_model_at_end,\n",
    "    eval_strategy=cnfg.evaluation_strategy,\n",
    "    logging_strategy=cnfg.logging_strategy,\n",
    "    save_strategy=cnfg.save_strategy,\n",
    "    output_dir=cnfg.output_dir,\n",
    "    report_to=cnfg.report_to,\n",
    "    run_name=cnfg.run_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:05:01.052654Z",
     "iopub.status.busy": "2025-04-04T08:05:01.052355Z",
     "iopub.status.idle": "2025-04-04T08:05:01.416561Z",
     "shell.execute_reply": "2025-04-04T08:05:01.415702Z",
     "shell.execute_reply.started": "2025-04-04T08:05:01.052630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:05:20.325263Z",
     "iopub.status.busy": "2025-04-04T08:05:20.324891Z",
     "iopub.status.idle": "2025-04-04T08:06:19.928172Z",
     "shell.execute_reply": "2025-04-04T08:06:19.926948Z",
     "shell.execute_reply.started": "2025-04-04T08:05:20.325235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Evaluate on a small subset for speed (adjust as needed)\n",
    "eval_samples = 100\n",
    "model.eval()\n",
    "device = model.device\n",
    "predictions, references = [], []\n",
    "exact_matches = 0\n",
    "\n",
    "test_subset = tokenized_dataset[\"test\"].select(range(eval_samples))\n",
    "\n",
    "for example in tqdm(test_subset):\n",
    "    input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=50,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
    "    ref = tokenizer.decode(example[\"labels\"], skip_special_tokens=True).strip()\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(ref)\n",
    "\n",
    "    if pred == ref:\n",
    "        exact_matches += 1\n",
    "\n",
    "# ROUGE\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "print(\"ROUGE scores:\")\n",
    "for k, v in rouge_result.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# BLEU\n",
    "bleu_result = bleu.compute(predictions=predictions, references=[[r] for r in references])\n",
    "print(f\"\\nBLEU score: {bleu_result['bleu']:.4f}\")\n",
    "\n",
    "# Accuracy (exact match)\n",
    "exact_match_accuracy = exact_matches / eval_samples\n",
    "print(f\"\\nExact Match Accuracy: {exact_match_accuracy:.4f}\")\n",
    "\n",
    "# Perplexity\n",
    "def calculate_perplexity(model, tokenizer, examples):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for example in tqdm(examples, desc=\"Perplexity\"):\n",
    "        input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "        labels = torch.tensor(example[\"labels\"]).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, labels=labels)\n",
    "            loss = output.loss\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    return np.exp(avg_loss)\n",
    "\n",
    "perplexity = calculate_perplexity(model, tokenizer, test_subset)\n",
    "print(f\"\\nPerplexity: {perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:25:22.361140Z",
     "iopub.status.busy": "2025-04-04T07:25:22.360585Z",
     "iopub.status.idle": "2025-04-04T07:25:23.333835Z",
     "shell.execute_reply": "2025-04-04T07:25:23.332781Z",
     "shell.execute_reply.started": "2025-04-04T07:25:22.361103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if test_run:\n",
    "    test_input = '''def <extra_id_0>(x, y):\n",
    "        return (x ** 2 + y ** 2) ** 0.5\n",
    "    '''\n",
    "    test_input2 = '''public static int <extra_id_0>(int n) {\n",
    "        if (n == 0) {\n",
    "            return 1;\n",
    "        }\n",
    "        return n * <extra_id_0>(n - 1);\n",
    "    }\n",
    "    '''\n",
    "    test_input3 = '''def <extra_id_0>(data, window_size=3):\n",
    "        if len(data) < window_size:\n",
    "            raise ValueError(\"Data length must be at least equal to the window size.\")\n",
    "        \n",
    "        moving_averages = []\n",
    "        for i in range(len(data) - window_size + 1):\n",
    "            window = data[i : i + window_size]\n",
    "            window_average = sum(window) / window_size\n",
    "            moving_averages.append(window_average)\n",
    "        \n",
    "        return moving_averages\n",
    "    '''\n",
    "    test_input4 = '''public static int <extra_id_0>(int[] numbers) {\n",
    "        int max = Integer.MIN_VALUE;\n",
    "        for (int num : numbers) {\n",
    "            if (num > max) {\n",
    "                max = num;\n",
    "            }\n",
    "        }\n",
    "        return max;\n",
    "    }\n",
    "    '''\n",
    "    # Tokenize (on GPU)\n",
    "    inputs = tokenizer(test_input2, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    generated_ids = model.generate(**inputs, max_length=16)\n",
    "    \n",
    "    # Decode\n",
    "    output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(\"Predicted method name:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    !zip -r /kaggle/working/training_checkpoints.zip /kaggle/working/debug_results\n",
    "else:\n",
    "    !zip -r /kaggle/working/training_checkpoints.zip /kaggle/working/training_results"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7046795,
     "sourceId": 11279631,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7047303,
     "sourceId": 11279702,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mng_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
